{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random, Distributions\n",
    "\n",
    "function xavier_init(input_dim::Int, output_dim::Union{Int,Nothing}=nothing)\n",
    "    if output_dim === nothing\n",
    "    scale = sqrt(2.0 / (input_dim + 1))\n",
    "        return rand(Normal(0, scale), input_dim)\n",
    "    end\n",
    "    scale = sqrt(2.0 / (input_dim + output_dim))\n",
    "    return rand(Normal(0, scale), input_dim, output_dim)\n",
    "end\n",
    "\n",
    "abstract type Node end\n",
    "\n",
    "mutable struct InputNode <: Node\n",
    "    output::AbstractVecOrMat\n",
    "    name::String\n",
    "\n",
    "    InputNode(output::AbstractVecOrMat; name=\"?\"::String) = new(output, name)\n",
    "    InputNode(output_size; name=\"?\"::String) = new(zeros(output_size...), name)\n",
    "end\n",
    "\n",
    "struct ConstantNode <: Node\n",
    "    output::AbstractVecOrMat\n",
    "    name::String\n",
    "    ConstantNode(output; name=\"?\"::String) = new([output], name)\n",
    "end\n",
    "\n",
    "mutable struct VariableNode <: Node\n",
    "    output::AbstractVecOrMat\n",
    "    gradient::AbstractVecOrMat\n",
    "    name::String\n",
    "\n",
    "    VariableNode(output_size::Tuple{Int,Int}; name=\"?\"::String) = new(xavier_init(output_size...), zeros(output_size...), name)\n",
    "    VariableNode(output::AbstractVecOrMat; name=\"?\"::String) = new(output, zeros(size(output)), name)\n",
    "end\n",
    "\n",
    "mutable struct OperationNode{F} <: Node\n",
    "    inputs::Vector{Node}\n",
    "    output::Union{AbstractVecOrMat,Nothing}\n",
    "    gradient::Union{AbstractVecOrMat,Nothing}\n",
    "    name::String\n",
    "    OperationNode(fun::F, inputs::Vector{Node}; name=\"?\"::String) where {F} =\n",
    "        new{F}(inputs, nothing, nothing, name)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traversing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_node! (generic function with 4 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "function visit(node::Node, visited::Set{Node}, order::Vector{Node})\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "    \n",
    "function visit(node::OperationNode, visited::Set{Node}, order::Vector{Node})\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function topological_sort(head::Node)\n",
    "    visited = Set{Node}()\n",
    "    order = Vector{Node}()\n",
    "    visit(head, visited, order)\n",
    "    return order\n",
    "end\n",
    "\n",
    "\n",
    "function init_nodes!(order::Vector{Node})\n",
    "    for node in order\n",
    "        init_node!(node)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "init_node!(node::ConstantNode) = nothing\n",
    "init_node!(node::InputNode) = nothing\n",
    "init_node!(node::VariableNode) = nothing\n",
    "function init_node!(node::OperationNode)\n",
    "    println(\"init_node! \", typeof(node))\n",
    "    println(\"input sizes: \", [(input.name,size(input.output)) for input in node.inputs])\n",
    "    println(\"number of inputs: \", length(node.inputs))\n",
    "    # println(\"inputs: \", [input.output for input in node.inputs])\n",
    "    output_size = size(forward(node, [input.output for input in node.inputs]...))\n",
    "    node.output = zeros(output_size)\n",
    "    node.gradient = zeros(output_size)\n",
    "    println(\"init_node-successful! \", typeof(node))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward / Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adjust! (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "reset!(node::ConstantNode) = nothing\n",
    "reset!(node::InputNode) = nothing\n",
    "reset!(node::VariableNode) = fill!(node.gradient, zero(eltype(node.gradient)))\n",
    "reset!(node::OperationNode) = fill!(node.gradient, zero(eltype(node.gradient)))\n",
    "function reset!(order::Vector{Node})\n",
    "    for node in order\n",
    "        reset!(node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "\n",
    "reset_operations!(node::ConstantNode) = nothing\n",
    "reset_operations!(node::InputNode) = nothing\n",
    "reset_operations!(node::VariableNode) = nothing\n",
    "reset_operations!(node::OperationNode) = fill!(node.gradient, zero(eltype(node.gradient)))\n",
    "\n",
    "\n",
    "compute!(node::ConstantNode) = nothing\n",
    "compute!(node::InputNode) = nothing\n",
    "compute!(node::VariableNode) = nothing\n",
    "function compute!(node::OperationNode)\n",
    "    node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "end\n",
    "\n",
    "function forward!(order::Vector{Node})\n",
    "    for node in order\n",
    "        compute!(node)\n",
    "        reset_operations!(node)\n",
    "    end\n",
    "    return last(order).output\n",
    "end\n",
    "\n",
    "\n",
    "update!(node::ConstantNode, gradient) = nothing\n",
    "update!(node::VariableNode, gradient) = let \n",
    "    if  isnothing(node.gradient)\n",
    "        node.gradient = gradient\n",
    "    else\n",
    "        node.gradient .+= gradient\n",
    "    end\n",
    "end\n",
    "update!(node::InputNode, gradient) = nothing\n",
    "update!(node::OperationNode, gradient) =\n",
    "    let\n",
    "        if  isnothing(node.gradient)\n",
    "            node.gradient = gradient\n",
    "        else\n",
    "            node.gradient .+= gradient\n",
    "        end\n",
    "    end\n",
    "\n",
    "function backward!(order::Vector{Node}; seed=1.0)\n",
    "    result = last(order)\n",
    "    result.gradient = [seed]\n",
    "    for node in reverse(order)\n",
    "        backward!(node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function backward!(node::ConstantNode) end\n",
    "function backward!(node::VariableNode) end\n",
    "function backward!(node::InputNode) end\n",
    "function backward!(node::OperationNode)\n",
    "    inputs = node.inputs\n",
    "    gradients = backward(node, [input.output for input in inputs]..., node.gradient)\n",
    "    for (input, gradient) in zip(inputs, gradients)\n",
    "        update!(input, gradient)\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 9 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: +\n",
    "+(x::Node, y::Node) = OperationNode(+, Node[x, y])\n",
    "forward(::OperationNode{typeof(+)}, x, y) = return x .+ y\n",
    "backward(::OperationNode{typeof(+)}, x, y, g) = tuple(g, g)\n",
    "\n",
    "\n",
    "import Base: -\n",
    "Base.Broadcast.broadcasted(-, x::Node, y::Node) = OperationNode(-, Node[x, y])\n",
    "forward(::OperationNode{typeof(-)}, x, y) = return x .- y\n",
    "backward(::OperationNode{typeof(-)}, x, y, g) = tuple(g, -g)\n",
    "\n",
    "\n",
    "import Base: *\n",
    "import LinearAlgebra: mul!\n",
    "# x * y (aka matrix multiplication)\n",
    "*(A::Node, x::Node) = OperationNode(mul!, Node[A, x])\n",
    "forward(::OperationNode{typeof(mul!)}, A, x) = return A * x\n",
    "backward(::OperationNode{typeof(mul!)}, A, x, g) = tuple(g * x', A' * g)\n",
    "\n",
    "\n",
    "# x .* y (element-wise multiplication)\n",
    "import Base: broadcast\n",
    "broadcasted(*, x::Node, y::Node) = OperationNode(*, Node[x, y])\n",
    "forward(::OperationNode{typeof(*)}, x, y) = return x .* y\n",
    "# backward(::OperationNode{typeof(*)}, x, y, g) = tuple(g .* y, g .* x)\n",
    "# backward(::OperationNode{typeof(*)}, x, y, g) = tuple(g .* y, x .* g)\n",
    "backward(node::OperationNode{typeof(*)}, x, y, g) =\n",
    "    let\n",
    "return tuple(g .* y, g .* x)\n",
    "    end\n",
    "\n",
    "import Base: sum\n",
    "sum(x::Node) = OperationNode(sum, Node[x])\n",
    "forward(::OperationNode{typeof(sum)}, x) = return [sum(x)]\n",
    "# backward(::OperationNode{typeof(sum)}, x, g) = tuple(g .* ones(size(x)))\n",
    "# JEBIE SIE NA BACKWARDZIE\n",
    "backward(::OperationNode{typeof(sum)}, x, g) =\n",
    "    let\n",
    "        𝟏 = ones(length(x))\n",
    "        J = 𝟏'\n",
    "        tuple(J' * g)\n",
    "    end\n",
    "\n",
    "\n",
    "import Base: ^\n",
    "^(x::Node, n::Node) = OperationNode(^, Node[x, n])\n",
    "forward(::OperationNode{typeof(^)}, x, n) = return x .^ n\n",
    "backward(::OperationNode{typeof(^)}, x, n, g) =\n",
    "    let\n",
    "        return tuple(g .* n .* x .^ (n .- 1), g .* log.(abs.(x)) .* x .^ n)\n",
    "    end\n",
    "\n",
    "\n",
    "# tanh function overload with forward and backward methods\n",
    "import Base: tanh\n",
    "tanh(x::Node) = OperationNode(tanh, Node[x])\n",
    "forward(::OperationNode{typeof(tanh)}, x) = return tanh.(x)\n",
    "backward(::OperationNode{typeof(tanh)}, x, g) = tuple(g .* (1 .- tanh.(x) .^ 2))\n",
    "\n",
    "# sigmoid function overload with forward and backward methods\n",
    "import Base: broadcast\n",
    "sigmoid(x::Node) = OperationNode(sigmoid, Node[x])\n",
    "forward(::OperationNode{typeof(sigmoid)}, x) = return sigmoid.(x)\n",
    "backward(::OperationNode{typeof(sigmoid)}, x, g) = tuple(g .* sigmoid.(x) .* (1 .- sigmoid.(x)))\n",
    "\n",
    "\n",
    "cross_entropy_loss(y_hat::Node, y::Node) = OperationNode(cross_entropy_loss, Node[y_hat, y])\n",
    "forward(::OperationNode{typeof(cross_entropy_loss)}, y_hat, y) =\n",
    "    let\n",
    "        y_hat = y_hat .- maximum(y_hat)\n",
    "        y_hat = exp.(y_hat) ./ sum(exp.(y_hat))\n",
    "        loss = sum(log.(y_hat) .* y) * -1.0\n",
    "        return [loss]\n",
    "    end\n",
    "backward(::OperationNode{typeof(cross_entropy_loss)}, y_hat, y, g) =\n",
    "    let\n",
    "        y_hat = y_hat .- maximum(y_hat)\n",
    "        y_hat = exp.(y_hat) ./ sum(exp.(y_hat))\n",
    "        return tuple(g .* (y_hat - y))\n",
    "    end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adjust! (generic function with 2 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mutable struct Network\n",
    "   inputs::Vector{InputNode}\n",
    "   \n",
    "   Wx::VariableNode\n",
    "   Wh::VariableNode\n",
    "   b::VariableNode\n",
    "   h::VariableNode\n",
    "   \n",
    "   Wy::VariableNode\n",
    "   by::VariableNode\n",
    "\n",
    "   desired_output::InputNode\n",
    "\n",
    "   output_graph::Vector{Node}\n",
    "   loss_graph::Vector{Node}\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function declare_RNN(input_lenght::Int, output_length::Int, neurons::Int)\n",
    "   inputs = Vector{InputNode}()\n",
    "\n",
    "      # Wx = VariableNode((input_lenght,neurons))\n",
    "      Wx = VariableNode((neurons, input_lenght),name=\"Wx\")\n",
    "      Wh = VariableNode((neurons, neurons),name=\"Wh\")\n",
    "      b = VariableNode((neurons,1),name=\"b\")\n",
    "      h = VariableNode(zeros(neurons), name=\"h\")\n",
    "      Wy = VariableNode((output_length, neurons),name=\"Wy\")\n",
    "      by = VariableNode((output_length,1),name=\"by\")\n",
    "      desired_output = InputNode(zeros(output_length), name=\"desired_output\")\n",
    "      output_graph = Vector{Node}()\n",
    "      loss_graph = Vector{Node}()\n",
    "      Network(inputs, Wx, Wh, b, h, Wy, by, desired_output, output_graph, loss_graph)\n",
    "   end\n",
    "\n",
    "function unfold!(network::Network, n_sequences::Int)\n",
    "      h = network.h\n",
    "      Wh = network.Wh\n",
    "      Wx = network.Wx\n",
    "      b = network.b\n",
    "      Wy = network.Wy\n",
    "      by = network.by\n",
    "      y = network.desired_output\n",
    "      output_graph = network.output_graph\n",
    "      loss_graph = network.loss_graph\n",
    "\n",
    "\n",
    "      for i in 1:n_sequences\n",
    "         x = InputNode(zeros(size(Wx.output)[2]), name=\"x\")\n",
    "         push!(network.inputs, x)\n",
    "         h = tanh((Wx * x) .+ (Wh * h) .+ b)\n",
    "      end\n",
    "      y_hat = (Wy * h) .+ by\n",
    "\n",
    "      network.output_graph= topological_sort(y_hat)\n",
    "      network.loss_graph = topological_sort(cross_entropy_loss(y_hat, y))\n",
    "      init_nodes!(network.loss_graph)\n",
    "end\n",
    "\n",
    "function feed_with_sequence!(network::Network, sequences...)\n",
    "      for (input, x_i) in zip(network.inputs, sequences)\n",
    "         input.output = x_i\n",
    "      end\n",
    "      return nothing\n",
    "end\n",
    "\n",
    "function feed_desired_output!(network::Network, y::AbstractVecOrMat)\n",
    "      network.desired_output.output = y\n",
    "      return nothing\n",
    "end\n",
    "\n",
    "function adjust!(net::Network, lr, batchsize)\n",
    "   net.Wx.output .-= lr .* net.Wx.gradient ./ batchsize\n",
    "   net.Wh.output .-= lr .* net.Wh.gradient ./ batchsize\n",
    "   net.b.output .-= lr .* net.b.gradient ./ batchsize\n",
    "   net.Wy.output .-= lr .* net.Wy.gradient ./ batchsize\n",
    "   net.by.output .-= lr .* net.by.gradient ./ batchsize\n",
    "   \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_and_accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss_and_accuracy(net::Network, data)\n",
    "    correct = 0\n",
    "    \n",
    "    for (X_batch, Y_batch) in loader(test_data, batchsize=length(test_data))\n",
    "    \n",
    "        for i in 1:length(test_data)\n",
    "            feed_with_sequence!(net,\n",
    "                X_batch[1:196, i:i],\n",
    "                X_batch[197:392, i:i],\n",
    "                X_batch[393:588, i:i],\n",
    "                X_batch[589:end, i:i]\n",
    "            )\n",
    "    \n",
    "            y = @views Y_batch[:,i]\n",
    "            ŷ = forward!(net.output_graph)\n",
    "    \n",
    "            if Flux.onecold(ŷ) == [Flux.onecold(y)]\n",
    "                correct +=1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    println(\"Correct: \", round(100 * correct/length(test_data); digits=2), \"%\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loader (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLDatasets, Flux\n",
    "train_data = MLDatasets.MNIST(split=:train)\n",
    "test_data  = MLDatasets.MNIST(split=:test)\n",
    "\n",
    "function loader(data; batchsize::Int=1)\n",
    "    x1dim = reshape(data.features, 28 * 28, :) # reshape 28×28 pixels into a vector of pixels\n",
    "    yhot  = Flux.onehotbatch(data.targets, 0:9) # make a 10×60000 OneHotMatrix\n",
    "    Flux.DataLoader((x1dim, yhot); batchsize, shuffle=true)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_node! OperationNode{typeof(mul!)}\n",
      "input sizes: Tuple{String, Tuple{Int64, Vararg{Int64}}}[(\"Wx\", (64, 196)), (\"x\", (196,))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(mul!)}\n",
      "init_node! OperationNode{typeof(mul!)}\n",
      "input sizes: Tuple{String, Tuple{Int64, Vararg{Int64}}}[(\"Wx\", (64, 196)), (\"x\", (196,))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(mul!)}\n",
      "init_node! OperationNode{typeof(mul!)}\n",
      "input sizes: Tuple{String, Tuple{Int64, Vararg{Int64}}}[(\"Wx\", (64, 196)), (\"x\", (196,))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(mul!)}\n",
      "init_node! OperationNode{typeof(mul!)}\n",
      "input sizes: Tuple{String, Tuple{Int64, Vararg{Int64}}}[(\"Wx\", (64, 196)), (\"x\", (196,))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(mul!)}\n",
      "init_node! OperationNode{typeof(mul!)}\n",
      "input sizes: Tuple{String, Tuple{Int64, Vararg{Int64}}}[(\"Wh\", (64, 64)), (\"h\", (64,))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(mul!)}\n",
      "init_node! OperationNode{typeof(+)}\n",
      "input sizes: [(\"?\", (64,)), (\"?\", (64,))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(+)}\n",
      "init_node! OperationNode{typeof(+)}\n",
      "input sizes: Tuple{String, Tuple{Int64, Vararg{Int64}}}[(\"?\", (64,)), (\"b\", (64, 1))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(+)}\n",
      "init_node! OperationNode{typeof(tanh)}\n",
      "input sizes: [(\"?\", (64, 1))]\n",
      "number of inputs: 1\n",
      "init_node-successful! OperationNode{typeof(tanh)}\n",
      "init_node! OperationNode{typeof(mul!)}\n",
      "input sizes: [(\"Wh\", (64, 64)), (\"?\", (64, 1))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(mul!)}\n",
      "init_node! OperationNode{typeof(+)}\n",
      "input sizes: Tuple{String, Tuple{Int64, Vararg{Int64}}}[(\"?\", (64,)), (\"?\", (64, 1))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(+)}\n",
      "init_node! OperationNode{typeof(+)}\n",
      "input sizes: [(\"?\", (64, 1)), (\"b\", (64, 1))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(+)}\n",
      "init_node! OperationNode{typeof(tanh)}\n",
      "input sizes: [(\"?\", (64, 1))]\n",
      "number of inputs: 1\n",
      "init_node-successful! OperationNode{typeof(tanh)}\n",
      "init_node! OperationNode{typeof(mul!)}\n",
      "input sizes: [(\"Wh\", (64, 64)), (\"?\", (64, 1))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(mul!)}\n",
      "init_node! OperationNode{typeof(+)}\n",
      "input sizes: Tuple{String, Tuple{Int64, Vararg{Int64}}}[(\"?\", (64,)), (\"?\", (64, 1))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(+)}\n",
      "init_node! OperationNode{typeof(+)}\n",
      "input sizes: [(\"?\", (64, 1)), (\"b\", (64, 1))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(+)}\n",
      "init_node! OperationNode{typeof(tanh)}\n",
      "input sizes: [(\"?\", (64, 1))]\n",
      "number of inputs: 1\n",
      "init_node-successful! OperationNode{typeof(tanh)}\n",
      "init_node! OperationNode{typeof(mul!)}\n",
      "input sizes: [(\"Wh\", (64, 64)), (\"?\", (64, 1))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(mul!)}\n",
      "init_node! OperationNode{typeof(+)}\n",
      "input sizes: Tuple{String, Tuple{Int64, Vararg{Int64}}}[(\"?\", (64,)), (\"?\", (64, 1))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(+)}\n",
      "init_node! OperationNode{typeof(+)}\n",
      "input sizes: [(\"?\", (64, 1)), (\"b\", (64, 1))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(+)}\n",
      "init_node! OperationNode{typeof(tanh)}\n",
      "input sizes: [(\"?\", (64, 1))]\n",
      "number of inputs: 1\n",
      "init_node-successful! OperationNode{typeof(tanh)}\n",
      "init_node! OperationNode{typeof(mul!)}\n",
      "input sizes: [(\"Wy\", (10, 64)), (\"?\", (64, 1))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(mul!)}\n",
      "init_node! OperationNode{typeof(+)}\n",
      "input sizes: [(\"?\", (10, 1)), (\"by\", (10, 1))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(+)}\n",
      "init_node! OperationNode{typeof(cross_entropy_loss)}\n",
      "input sizes: Tuple{String, Tuple{Int64, Vararg{Int64}}}[(\"?\", (10, 1)), (\"desired_output\", (10,))]\n",
      "number of inputs: 2\n",
      "init_node-successful! OperationNode{typeof(cross_entropy_loss)}\n"
     ]
    }
   ],
   "source": [
    "n_sequences = 4\n",
    "\n",
    "net = declare_RNN(14*14, 10, 64)\n",
    "unfold!(net, n_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(epochs = 5, batchsize = 100, lr = 0.05)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "settings = (;\n",
    "    epochs = 5,\n",
    "    batchsize = 100,\n",
    "    lr = 0.05\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      " 30.599268 seconds (41.73 M allocations: 34.225 GiB, 5.23% gc time)\n",
      "Current loss: 17310.409321171443\n",
      "Correct: 93.07%\n",
      "Epoch: 2\n",
      " 30.595451 seconds (41.73 M allocations: 34.225 GiB, 5.45% gc time)\n",
      "Current loss: 13776.991677565367\n",
      "Correct: 94.09%\n",
      "Epoch: 3\n",
      " 30.138373 seconds (41.73 M allocations: 34.225 GiB, 5.41% gc time)\n",
      "Current loss: 11716.25195472323\n",
      "Correct: 94.82%\n",
      "Epoch: 4\n",
      " 28.750153 seconds (41.73 M allocations: 34.225 GiB, 5.34% gc time)\n",
      "Current loss: 10259.146006783369\n",
      "Correct: 95.16%\n",
      "Epoch: 5\n",
      " 28.798814 seconds (41.73 M allocations: 34.225 GiB, 5.42% gc time)\n",
      "Current loss: 9192.107041483754\n",
      "Correct: 95.54%\n",
      "155.207963 seconds (217.15 M allocations: 172.522 GiB, 5.22% gc time, 0.33% compilation time)\n"
     ]
    }
   ],
   "source": [
    "loss_in_epoch = 0.0\n",
    "losses = []\n",
    "\n",
    "graph = net.loss_graph\n",
    "\n",
    "@time for epoch in 1:settings.epochs\n",
    "    println(\"Epoch: \", epoch)\n",
    "    reset!(graph)\n",
    "    loss_in_epoch = 0.0\n",
    "    @time for (x, y) in loader(train_data, batchsize=settings.batchsize)\n",
    "        reset!(graph)\n",
    "        for i in 1:settings.batchsize\n",
    "            feed_with_sequence!(net,\n",
    "                view(x, 1:196,i),\n",
    "                view(x, 197:392,i),\n",
    "                view(x, 393:588,i),\n",
    "                view(x, 589:784,i))\n",
    "            feed_desired_output!(net, view(y, :, i))\n",
    "            forward!(graph)\n",
    "            loss = forward!(graph)\n",
    "            loss_in_epoch += first(loss)\n",
    "            backward!(graph)\n",
    "        end\n",
    "        adjust!(net, settings.lr, settings.batchsize)\n",
    "    end\n",
    "    println(\"Current loss: \", loss_in_epoch)\n",
    "    push!(losses, first(loss_in_epoch))\n",
    "    loss_and_accuracy(net, train_data)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_in_epoch = 0.0\n",
    "# losses = []\n",
    "\n",
    "# graph = net.loss_graph\n",
    "\n",
    "# @time for epoch in 1:settings.epochs\n",
    "#     loss_in_epoch = 0.0\n",
    "#     println(\"Epoch: \", epoch)\n",
    "#     reset!(graph)\n",
    "#     @time for (x, y) in loader(train_data, batchsize=settings.batchsize)\n",
    "#         feed_with_sequence!(net,\n",
    "#             x[1:196, :],\n",
    "#             x[197:392, :],\n",
    "#             x[393:588, :],\n",
    "#             x[589:784, :]\n",
    "#         )\n",
    "#         feed_desired_output!(net, y)\n",
    "#         currentloss = forward!(graph)\n",
    "#         backward!(graph)\n",
    "#         loss_in_epoch += first(currentloss)\n",
    "#         end\n",
    "#     # foreach(x -> x.gradient ./= (length(train_data)/settings.batchsize), net.variables)\n",
    "# adjust!(net, settings.lr, length(train_data))\n",
    "#     # @show net.variables[1].gradient\n",
    "#     println(\"Current loss: \", loss_in_epoch)\n",
    "#     push!(losses, first(loss_in_epoch))\n",
    "#     println(\"Accuracy: \", loss_and_accuracy(net, train_data))\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.106907 seconds (11.02 k allocations: 181.709 MiB, 5.28% gc time)\n",
      "i = 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "@time for (x, y) in loader(train_data, batchsize=settings.batchsize)\n",
    "i+=1\n",
    "end\n",
    "@show i\n",
    "length(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_and_accuracy(net::Network, data)\n",
    "    correct = 0\n",
    "    graph = net.output_graph\n",
    "\n",
    "    for(x,y) in loader(data; batchsize=length(data))\n",
    "        for i in 1:length(x)\n",
    "            feed_with_sequence!(net,\n",
    "            view(x, 1:196,i),\n",
    "            view(x, 197:392,i),\n",
    "            view(x, 393:588,i),\n",
    "            view(x, 589:784,i)\n",
    "            )\n",
    "\n",
    "            y = y[:,i]\n",
    "            ŷ = forward!(graph)\n",
    "    \n",
    "            if Flux.onecold(ŷ) == [Flux.onecold(y)]\n",
    "                correct +=1\n",
    "            end\n",
    "\n",
    "        end\n",
    "    end\n",
    "    println(\"Correct: \", round(100 * correct/length(data); digits=2), \"%\")\n",
    "end\n",
    "\n",
    "loss_and_accuracy(net, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 91.72%\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
