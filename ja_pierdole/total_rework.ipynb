{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation of RNN network\n",
    "\n",
    "##### Author: Michał Tomczyk 311524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random, Distributions\n",
    "\n",
    "function xavier_init(input_dim::Int, output_dim::Union{Int,Nothing}=nothing)\n",
    "    if output_dim === nothing\n",
    "    scale = sqrt(2.0 / (input_dim + 1))\n",
    "        return rand(Normal(0, scale), input_dim)\n",
    "    end\n",
    "    scale = sqrt(2.0 / (input_dim + output_dim))\n",
    "    return rand(Normal(0, scale), input_dim, output_dim)\n",
    "end\n",
    "\n",
    "abstract type Node end\n",
    "\n",
    "mutable struct InputNode <: Node\n",
    "    output::AbstractVecOrMat\n",
    "    name::String\n",
    "\n",
    "    InputNode(output::AbstractVecOrMat; name=\"?\"::String) = new(output, name)\n",
    "    InputNode(output_size; name=\"?\"::String) = new(zeros(output_size...), name)\n",
    "end\n",
    "\n",
    "struct ConstantNode <: Node\n",
    "    output::AbstractVecOrMat\n",
    "    name::String\n",
    "    ConstantNode(output; name=\"?\"::String) = new([output], name)\n",
    "end\n",
    "\n",
    "mutable struct VariableNode <: Node\n",
    "    output::AbstractVecOrMat\n",
    "    gradient::AbstractVecOrMat\n",
    "    name::String\n",
    "\n",
    "    VariableNode(output_size::Tuple{Int,Int}; name=\"?\"::String) = new(xavier_init(output_size...), zeros(output_size...), name)\n",
    "    VariableNode(output::AbstractVecOrMat; name=\"?\"::String) = new(output, zeros(size(output)), name)\n",
    "end\n",
    "\n",
    "mutable struct OperationNode{F} <: Node\n",
    "    inputs::Vector{Node}\n",
    "    output::Union{AbstractVecOrMat,Nothing}\n",
    "    gradient::Union{AbstractVecOrMat,Nothing}\n",
    "    name::String\n",
    "    OperationNode(fun::F, inputs::Vector{Node}; name=\"?\"::String) where {F} =\n",
    "        new{F}(inputs, nothing, nothing, name)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traversing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_node! (generic function with 4 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "function visit(node::Node, visited::Set{Node}, order::Vector{Node})\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "    \n",
    "function visit(node::OperationNode, visited::Set{Node}, order::Vector{Node})\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function topological_sort(head::Node)\n",
    "    visited = Set{Node}()\n",
    "    order = Vector{Node}()\n",
    "    visit(head, visited, order)\n",
    "    return order\n",
    "end\n",
    "\n",
    "\n",
    "function init_nodes!(order::Vector{Node})\n",
    "    for node in order\n",
    "        init_node!(node)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "init_node!(node::ConstantNode) = nothing\n",
    "init_node!(node::InputNode) = nothing\n",
    "init_node!(node::VariableNode) = nothing\n",
    "function init_node!(node::OperationNode)\n",
    "    # println(\"init_node! \", typeof(node))\n",
    "    # println(\"input sizes: \", [(input.name,size(input.output)) for input in node.inputs])\n",
    "    # println(\"number of inputs: \", length(node.inputs))\n",
    "    # println(\"inputs: \", [input.output for input in node.inputs])\n",
    "    output_size = size(forward(node, [input.output for input in node.inputs]...))\n",
    "    node.output = zeros(output_size)\n",
    "    node.gradient = zeros(output_size)\n",
    "    # println(\"init_node-successful! \", typeof(node))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward / Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 5 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "reset!(node::ConstantNode) = nothing\n",
    "reset!(node::InputNode) = nothing\n",
    "reset!(node::VariableNode) = fill!(node.gradient, zero(eltype(node.gradient)))\n",
    "reset!(node::OperationNode) = fill!(node.gradient, zero(eltype(node.gradient)))\n",
    "function reset!(order::Vector{Node})\n",
    "    for node in order\n",
    "        reset!(node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "\n",
    "reset_operations!(node::ConstantNode) = nothing\n",
    "reset_operations!(node::InputNode) = nothing\n",
    "reset_operations!(node::VariableNode) = nothing\n",
    "reset_operations!(node::OperationNode) = fill!(node.gradient, zero(eltype(node.gradient)))\n",
    "\n",
    "\n",
    "compute!(node::ConstantNode) = nothing\n",
    "compute!(node::InputNode) = nothing\n",
    "compute!(node::VariableNode) = nothing\n",
    "function compute!(node::OperationNode)\n",
    "    node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "end\n",
    "\n",
    "function forward!(order::Vector{Node})\n",
    "    for node in order\n",
    "        compute!(node)\n",
    "        reset_operations!(node)\n",
    "    end\n",
    "    return last(order).output\n",
    "end\n",
    "\n",
    "\n",
    "update!(node::ConstantNode, gradient) = nothing\n",
    "update!(node::VariableNode, gradient) = let \n",
    "    # println(\"node: \", size(node.gradient))\n",
    "    # println(\"gradient: \", size(gradient))\n",
    "if length(size(node.gradient)) == 1 || size(node.gradient)[2]==1\n",
    "    # println(\"bias!\")\n",
    "    # println(\"summed: \",size(sum(gradient,dims=2)))\n",
    "    if  isnothing(node.gradient)\n",
    "        # println(\"nothing!\")\n",
    "        node.gradient = sum(gradient,dims=2)\n",
    "    else\n",
    "        # println(\"appending!\")\n",
    "        node.gradient .+= sum(gradient,dims=2)\n",
    "    end\n",
    "else\n",
    "    if  isnothing(node.gradient)\n",
    "        node.gradient = gradient\n",
    "    else\n",
    "        node.gradient .+= gradient\n",
    "    end\n",
    "end\n",
    "# println(\"done!\")\n",
    "end\n",
    "update!(node::InputNode, gradient) = nothing\n",
    "update!(node::OperationNode, gradient) =\n",
    "    let\n",
    "        # println(\"node: \", size(node.gradient))\n",
    "        # println(\"gradient: \", size(gradient))\n",
    "        if length(size(node.gradient)) == 1 || size(node.gradient)[2]==1\n",
    "            # println(\"bias!\")\n",
    "            # println(\"summed: \",size(sum(gradient,dims=2)))\n",
    "            if  isnothing(node.gradient)\n",
    "                # println(\"nothing!\")\n",
    "                node.gradient = sum(gradient,dims=2)\n",
    "            else\n",
    "                # println(\"appending!\")\n",
    "                node.gradient .+= sum(gradient,dims=2)\n",
    "            end\n",
    "        else\n",
    "            if  isnothing(node.gradient)\n",
    "                node.gradient = gradient\n",
    "            else\n",
    "                node.gradient .+= gradient\n",
    "            end\n",
    "        end\n",
    "# println(\"done!\")\n",
    "\n",
    "    end\n",
    "\n",
    "function backward!(order::Vector{Node}; seed=1.0)\n",
    "    result = last(order)\n",
    "    result.gradient = [seed]\n",
    "    for node in reverse(order)\n",
    "        backward!(node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function backward!(node::ConstantNode) end\n",
    "function backward!(node::VariableNode) end\n",
    "function backward!(node::InputNode) end\n",
    "function backward!(node::OperationNode)\n",
    "    inputs = node.inputs\n",
    "    gradients = backward(node, [input.output for input in inputs]..., node.gradient)\n",
    "    for (input, gradient) in zip(inputs, gradients)\n",
    "        update!(input, gradient)\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 9 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: +\n",
    "+(x::Node, y::Node) = OperationNode(+, Node[x, y])\n",
    "forward(::OperationNode{typeof(+)}, x, y) = return x .+ y\n",
    "backward(::OperationNode{typeof(+)}, x, y, g) = tuple(g, g)\n",
    "\n",
    "\n",
    "import Base: -\n",
    "Base.Broadcast.broadcasted(-, x::Node, y::Node) = OperationNode(-, Node[x, y])\n",
    "forward(::OperationNode{typeof(-)}, x, y) = return x .- y\n",
    "backward(::OperationNode{typeof(-)}, x, y, g) = tuple(g, -g)\n",
    "\n",
    "\n",
    "import Base: *\n",
    "import LinearAlgebra: mul!\n",
    "# x * y (aka matrix multiplication)\n",
    "*(A::Node, x::Node) = OperationNode(mul!, Node[A, x])\n",
    "forward(::OperationNode{typeof(mul!)}, A, x) = return A * x\n",
    "backward(::OperationNode{typeof(mul!)}, A, x, g) = tuple(g * x', A' * g)\n",
    "\n",
    "\n",
    "# x .* y (element-wise multiplication)\n",
    "import Base: broadcast\n",
    "broadcasted(*, x::Node, y::Node) = OperationNode(*, Node[x, y])\n",
    "forward(::OperationNode{typeof(*)}, x, y) = return x .* y\n",
    "# backward(::OperationNode{typeof(*)}, x, y, g) = tuple(g .* y, g .* x)\n",
    "# backward(::OperationNode{typeof(*)}, x, y, g) = tuple(g .* y, x .* g)\n",
    "backward(node::OperationNode{typeof(*)}, x, y, g) =\n",
    "    let\n",
    "return tuple(g .* y, g .* x)\n",
    "    end\n",
    "\n",
    "import Base: sum\n",
    "sum(x::Node) = OperationNode(sum, Node[x])\n",
    "forward(::OperationNode{typeof(sum)}, x) = return [sum(x)]\n",
    "# backward(::OperationNode{typeof(sum)}, x, g) = tuple(g .* ones(size(x)))\n",
    "# JEBIE SIE NA BACKWARDZIE\n",
    "backward(::OperationNode{typeof(sum)}, x, g) =\n",
    "    let\n",
    "        𝟏 = ones(length(x))\n",
    "        J = 𝟏'\n",
    "        tuple(J' * g)\n",
    "    end\n",
    "\n",
    "\n",
    "import Base: ^\n",
    "^(x::Node, n::Node) = OperationNode(^, Node[x, n])\n",
    "forward(::OperationNode{typeof(^)}, x, n) = return x .^ n\n",
    "backward(::OperationNode{typeof(^)}, x, n, g) =\n",
    "    let\n",
    "        return tuple(g .* n .* x .^ (n .- 1), g .* log.(abs.(x)) .* x .^ n)\n",
    "    end\n",
    "\n",
    "\n",
    "# tanh function overload with forward and backward methods\n",
    "import Base: tanh\n",
    "tanh(x::Node) = OperationNode(tanh, Node[x])\n",
    "forward(::OperationNode{typeof(tanh)}, x) = return tanh.(x)\n",
    "backward(::OperationNode{typeof(tanh)}, x, g) = tuple(g .* (1 .- tanh.(x) .^ 2))\n",
    "\n",
    "# sigmoid function overload with forward and backward methods\n",
    "import Base: broadcast\n",
    "sigmoid(x::Node) = OperationNode(sigmoid, Node[x])\n",
    "forward(::OperationNode{typeof(sigmoid)}, x) = return sigmoid.(x)\n",
    "backward(::OperationNode{typeof(sigmoid)}, x, g) = tuple(g .* sigmoid.(x) .* (1 .- sigmoid.(x)))\n",
    "\n",
    "\n",
    "cross_entropy_loss(y_hat::Node, y::Node) = OperationNode(cross_entropy_loss, Node[y_hat, y])\n",
    "forward(::OperationNode{typeof(cross_entropy_loss)}, y_hat, y) =\n",
    "    let\n",
    "        y_hat = y_hat .- maximum(y_hat, dims=1)\n",
    "        y_hat = exp.(y_hat) ./ sum(exp.(y_hat), dims=1)\n",
    "        loss = sum(log.(y_hat) .* y, dims=1) * -1.0\n",
    "        return loss\n",
    "    end\n",
    "backward(::OperationNode{typeof(cross_entropy_loss)}, y_hat, y, g) =\n",
    "    let\n",
    "        y_hat = y_hat .- maximum(y_hat, dims=1)\n",
    "        y_hat = exp.(y_hat) ./ sum(exp.(y_hat), dims=1)\n",
    "        return tuple(g .* (y_hat - y))\n",
    "    end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adjust! (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mutable struct Network\n",
    "   inputs::Vector{InputNode}\n",
    "   \n",
    "   Wx::VariableNode\n",
    "   Wh::VariableNode\n",
    "   b::VariableNode\n",
    "   h::VariableNode\n",
    "   \n",
    "   Wy::VariableNode\n",
    "   by::VariableNode\n",
    "\n",
    "   desired_output::InputNode\n",
    "\n",
    "   output_graph::Vector{Node}\n",
    "   loss_graph::Vector{Node}\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function declare_RNN(input_lenght::Int, output_length::Int, neurons::Int)\n",
    "   inputs = Vector{InputNode}()\n",
    "\n",
    "      # Wx = VariableNode((input_lenght,neurons))\n",
    "      Wx = VariableNode((neurons, input_lenght),name=\"Wx\")\n",
    "      Wh = VariableNode((neurons, neurons),name=\"Wh\")\n",
    "      b = VariableNode((neurons,1),name=\"b\")\n",
    "      h = VariableNode(zeros(neurons,1), name=\"h\")\n",
    "      Wy = VariableNode((output_length, neurons),name=\"Wy\")\n",
    "      by = VariableNode((output_length,1),name=\"by\")\n",
    "      desired_output = InputNode(zeros(output_length), name=\"desired_output\")\n",
    "      output_graph = Vector{Node}()\n",
    "      loss_graph = Vector{Node}()\n",
    "      Network(inputs, Wx, Wh, b, h, Wy, by, desired_output, output_graph, loss_graph)\n",
    "   end\n",
    "\n",
    "function unfold!(network::Network, n_sequences::Int; batchsize=1)\n",
    "      h = network.h\n",
    "      Wh = network.Wh\n",
    "      Wx = network.Wx\n",
    "      b = network.b\n",
    "      Wy = network.Wy\n",
    "      by = network.by\n",
    "      y = network.desired_output\n",
    "      output_graph = network.output_graph\n",
    "      loss_graph = network.loss_graph\n",
    "\n",
    "\n",
    "      for i in 1:n_sequences\n",
    "         x = InputNode(zeros((size(Wx.output)[2],batchsize)), name=\"x\")\n",
    "         push!(network.inputs, x)\n",
    "         h = tanh((Wx * x) .+ (Wh * h) .+ b)\n",
    "      end\n",
    "      y_hat = (Wy * h) .+ by\n",
    "\n",
    "      network.output_graph= topological_sort(y_hat)\n",
    "      network.loss_graph = topological_sort(cross_entropy_loss(y_hat, y))\n",
    "      init_nodes!(network.loss_graph)\n",
    "end\n",
    "\n",
    "function feed_with_sequence!(network::Network, sequences...)\n",
    "      for (input, x_i) in zip(network.inputs, sequences)\n",
    "         input.output = x_i\n",
    "      end\n",
    "      return nothing\n",
    "end\n",
    "\n",
    "function feed_desired_output!(network::Network, y::AbstractVecOrMat)\n",
    "      network.desired_output.output = y\n",
    "      return nothing\n",
    "end\n",
    "\n",
    "function adjust!(net::Network, lr, batchsize)\n",
    "   net.Wx.output .-= lr .* (net.Wx.gradient ./ batchsize)\n",
    "   net.Wh.output .-= lr .* (net.Wh.gradient ./ batchsize)\n",
    "   net.b.output .-= lr .* (net.b.gradient ./ batchsize)\n",
    "   net.Wy.output .-= lr .* (net.Wy.gradient ./ batchsize)\n",
    "   net.by.output .-= lr .* (net.by.gradient ./ batchsize)\n",
    "   \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function calculate_accuracy(net::Network, data)\n",
    "    correct = 0\n",
    "    graph = net.output_graph\n",
    "\n",
    "    for (x, y) in loader(data, batchsize=settings.batchsize)\n",
    "            feed_with_sequence!(net,\n",
    "                view(x, 1:196, :),\n",
    "                view(x, 197:392, :),\n",
    "                view(x, 393:588, :),\n",
    "                view(x, 589:784, :)\n",
    "            )\n",
    "\n",
    "            y = y[:, :]\n",
    "            ŷ = forward!(graph)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            for i in 1:size(y, 2)\n",
    "                if Flux.onecold(ŷ[:, i]) == Flux.onecold(y[:, i])\n",
    "                    correct += 1\n",
    "                end\n",
    "            end\n",
    "    end\n",
    "    println(\"Correct: \", round(100 * correct / length(data); digits=2), \"%\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loader (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLDatasets, Flux\n",
    "train_data = MLDatasets.MNIST(split=:train)\n",
    "test_data  = MLDatasets.MNIST(split=:test)\n",
    "\n",
    "function loader(data; batchsize::Int=1)\n",
    "    x1dim = reshape(data.features, 28 * 28, :) # reshape 28×28 pixels into a vector of pixels\n",
    "    yhot  = Flux.onehotbatch(data.targets, 0:9) # make a 10×60000 OneHotMatrix\n",
    "    Flux.DataLoader((x1dim, yhot); batchsize, shuffle=true)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(epochs = 5, batchsize = 100, lr = 0.05)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = (;\n",
    "    epochs = 5,\n",
    "    batchsize = 100,\n",
    "    lr = 0.05\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sequences = 4\n",
    "\n",
    "net = declare_RNN(14*14, 10, 64)\n",
    "unfold!(net, n_sequences,batchsize=settings.batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 15.85%\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(net, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      " 11.927221 seconds (3.70 M allocations: 3.502 GiB, 2.44% gc time, 26.74% compilation time)\n",
      "Current loss: 35727.77719722139\n",
      "train: Correct: 90.6%\n",
      "test: Correct: 90.97%\n",
      "Epoch: 2\n",
      "  8.480574 seconds (544.33 k allocations: 3.296 GiB, 2.11% gc time)\n",
      "Current loss: 17615.490734986957\n",
      "train: Correct: 92.79%\n",
      "test: Correct: 92.99%\n",
      "Epoch: 3\n",
      "  8.380669 seconds (544.33 k allocations: 3.296 GiB, 2.11% gc time)\n",
      "Current loss: 14019.592758277879\n",
      "train: Correct: 94.09%\n",
      "test: Correct: 94.3%\n",
      "Epoch: 4\n",
      "  8.442271 seconds (544.33 k allocations: 3.296 GiB, 2.15% gc time)\n",
      "Current loss: 11930.279809664424\n",
      "train: Correct: 94.88%\n",
      "test: Correct: 94.83%\n",
      "Epoch: 5\n",
      "  8.202175 seconds (544.33 k allocations: 3.296 GiB, 2.17% gc time)\n",
      "Current loss: 10506.721461909621\n",
      "train: Correct: 95.35%\n",
      "test: Correct: 95.22%\n",
      " 59.416249 seconds (8.01 M allocations: 23.091 GiB, 2.34% gc time, 5.53% compilation time)\n"
     ]
    }
   ],
   "source": [
    "loss_in_epoch = 0.0\n",
    "losses = []\n",
    "\n",
    "graph = net.loss_graph\n",
    "\n",
    "@time for epoch in 1:settings.epochs\n",
    "    println(\"Epoch: \", epoch)\n",
    "    reset!(graph)\n",
    "    loss_in_epoch = 0.0\n",
    "    @time for (x, y) in loader(train_data, batchsize=settings.batchsize)\n",
    "        reset!(graph)\n",
    "            feed_with_sequence!(net,\n",
    "                view(x, 1:196,:),\n",
    "                view(x, 197:392,:),\n",
    "                view(x, 393:588,:),\n",
    "                view(x, 589:784,:))\n",
    "            feed_desired_output!(net, y)\n",
    "            forward!(graph)\n",
    "            loss = forward!(graph)\n",
    "            loss_in_epoch += sum(loss)\n",
    "            backward!(graph)\n",
    "        adjust!(net, settings.lr, settings.batchsize)\n",
    "    end\n",
    "    println(\"Current loss: \", loss_in_epoch)\n",
    "    push!(losses, first(loss_in_epoch))\n",
    "    print(\"train: \")\n",
    "    calculate_accuracy(net, train_data)\n",
    "    print(\"test: \")\n",
    "    \n",
    "    calculate_accuracy(net, test_data)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 95.22%\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(net, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
